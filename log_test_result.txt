2016-11-27 11:59:18,149 - INFO - Training Begins
2016-11-27 11:59:50,526 - DEBUG - Epoch:     1;  cost=2.333469391
2016-11-27 11:59:50,573 - DEBUG - Accuracy(training batch):0.27344
2016-11-27 12:00:00,490 - INFO - Accuracy - Full Training:nan
2016-11-27 12:00:02,909 - INFO - Accuracy - Full Validation:nan
2016-11-27 12:00:30,569 - DEBUG - Epoch:     2;  cost=1.727293015
2016-11-27 12:00:30,605 - DEBUG - Accuracy(training batch):0.42188
2016-11-27 12:01:00,320 - DEBUG - Epoch:     3;  cost=1.478715777
2016-11-27 12:01:00,358 - DEBUG - Accuracy(training batch):0.49219
2016-11-27 12:01:30,051 - DEBUG - Epoch:     4;  cost=1.079595208
2016-11-27 12:01:30,098 - DEBUG - Accuracy(training batch):0.58594
2016-11-27 12:02:00,303 - DEBUG - Epoch:     5;  cost=0.659602225
2016-11-27 12:02:00,340 - DEBUG - Accuracy(training batch):0.78906
2016-11-27 12:02:30,098 - DEBUG - Epoch:     6;  cost=0.371176511
2016-11-27 12:02:30,147 - DEBUG - Accuracy(training batch):0.87500
2016-11-27 12:03:01,063 - DEBUG - Epoch:     7;  cost=0.283016086
2016-11-27 12:03:01,103 - DEBUG - Accuracy(training batch):0.89844
2016-11-27 12:03:28,895 - DEBUG - Epoch:     8;  cost=0.244554698
2016-11-27 12:03:28,930 - DEBUG - Accuracy(training batch):0.92969
2016-11-27 12:03:58,550 - DEBUG - Epoch:     9;  cost=0.222434521
2016-11-27 12:03:58,585 - DEBUG - Accuracy(training batch):0.92969
2016-11-27 12:04:28,471 - DEBUG - Epoch:    10;  cost=0.162693650
2016-11-27 12:04:28,507 - DEBUG - Accuracy(training batch):0.96094
2016-11-27 12:04:59,297 - DEBUG - Epoch:    11;  cost=0.110810429
2016-11-27 12:04:59,334 - DEBUG - Accuracy(training batch):0.96094
2016-11-27 12:05:31,686 - DEBUG - Epoch:    12;  cost=0.065345220
2016-11-27 12:05:31,729 - DEBUG - Accuracy(training batch):0.97656
2016-11-27 12:06:03,637 - DEBUG - Epoch:    13;  cost=0.088245869
2016-11-27 12:06:03,684 - DEBUG - Accuracy(training batch):0.95312
2016-11-27 12:06:35,420 - DEBUG - Epoch:    14;  cost=0.074400507
2016-11-27 12:06:35,461 - DEBUG - Accuracy(training batch):0.96875
2016-11-27 12:07:06,546 - DEBUG - Epoch:    15;  cost=0.048954312
2016-11-27 12:07:06,587 - DEBUG - Accuracy(training batch):0.98438
2016-11-27 12:07:37,914 - DEBUG - Epoch:    16;  cost=0.080887593
2016-11-27 12:07:37,955 - DEBUG - Accuracy(training batch):0.97656
2016-11-27 12:08:10,934 - DEBUG - Epoch:    17;  cost=0.013827713
2016-11-27 12:08:10,982 - DEBUG - Accuracy(training batch):1.00000
2016-11-27 12:08:42,404 - DEBUG - Epoch:    18;  cost=0.025801931
2016-11-27 12:08:42,440 - DEBUG - Accuracy(training batch):0.98438
2016-11-27 12:09:12,563 - DEBUG - Epoch:    19;  cost=0.032091029
2016-11-27 12:09:12,598 - DEBUG - Accuracy(training batch):0.99219
2016-11-27 12:09:43,931 - DEBUG - Epoch:    20;  cost=0.008807890
2016-11-27 12:09:43,967 - DEBUG - Accuracy(training batch):1.00000
2016-11-27 12:09:44,004 - INFO - Optimization Finished!
2016-11-27 12:09:54,177 - INFO - Accuracy - Full Training:nan
2016-11-27 12:09:57,054 - INFO - Accuracy - Full Validation:nan
2016-11-27 13:42:57,759 - INFO - Training Begins
2016-11-27 13:43:17,614 - DEBUG - Epoch:     1;  cost=2.070500612
2016-11-27 13:43:17,665 - DEBUG - Accuracy(training batch):0.28000
2016-11-27 13:43:23,955 - INFO - Accuracy - Full Training:0.33175
2016-11-27 13:43:25,551 - INFO - Accuracy - Full Validation:0.34107
2016-11-27 13:43:44,625 - DEBUG - Epoch:     2;  cost=1.525725961
2016-11-27 13:43:44,644 - DEBUG - Accuracy(training batch):0.42000
2016-11-27 13:44:05,719 - DEBUG - Epoch:     3;  cost=1.321659565
2016-11-27 13:44:05,741 - DEBUG - Accuracy(training batch):0.52000
2016-11-27 13:44:25,024 - DEBUG - Epoch:     4;  cost=1.107135653
2016-11-27 13:44:25,043 - DEBUG - Accuracy(training batch):0.60000
2016-11-27 13:44:44,060 - DEBUG - Epoch:     5;  cost=0.665615976
2016-11-27 13:44:44,080 - DEBUG - Accuracy(training batch):0.77000
2016-11-27 13:45:03,405 - DEBUG - Epoch:     6;  cost=0.416043997
2016-11-27 13:45:03,429 - DEBUG - Accuracy(training batch):0.87000
2016-11-27 13:45:24,202 - DEBUG - Epoch:     7;  cost=0.356515884
2016-11-27 13:45:24,221 - DEBUG - Accuracy(training batch):0.91000
2016-11-27 13:45:43,563 - DEBUG - Epoch:     8;  cost=0.285426289
2016-11-27 13:45:43,583 - DEBUG - Accuracy(training batch):0.92000
2016-11-27 13:46:03,070 - DEBUG - Epoch:     9;  cost=0.171980247
2016-11-27 13:46:03,089 - DEBUG - Accuracy(training batch):0.95000
2016-11-27 13:46:22,958 - DEBUG - Epoch:    10;  cost=0.144125372
2016-11-27 13:46:22,983 - DEBUG - Accuracy(training batch):0.94000
2016-11-27 13:46:43,790 - DEBUG - Epoch:    11;  cost=0.072607413
2016-11-27 13:46:43,809 - DEBUG - Accuracy(training batch):0.98000
2016-11-27 13:47:03,531 - DEBUG - Epoch:    12;  cost=0.029833343
2016-11-27 13:47:03,556 - DEBUG - Accuracy(training batch):0.99000
2016-11-27 13:47:23,305 - DEBUG - Epoch:    13;  cost=0.029328194
2016-11-27 13:47:23,324 - DEBUG - Accuracy(training batch):0.99000
2016-11-27 13:47:43,645 - DEBUG - Epoch:    14;  cost=0.020713976
2016-11-27 13:47:43,666 - DEBUG - Accuracy(training batch):0.99000
2016-11-27 13:48:04,716 - DEBUG - Epoch:    15;  cost=0.015288215
2016-11-27 13:48:04,736 - DEBUG - Accuracy(training batch):1.00000
2016-11-27 13:48:24,762 - DEBUG - Epoch:    16;  cost=0.064370662
2016-11-27 13:48:24,781 - DEBUG - Accuracy(training batch):0.98000
2016-11-27 13:48:44,757 - DEBUG - Epoch:    17;  cost=0.041201886
2016-11-27 13:48:44,776 - DEBUG - Accuracy(training batch):0.98000
2016-11-27 13:49:05,767 - DEBUG - Epoch:    18;  cost=0.064674795
2016-11-27 13:49:05,791 - DEBUG - Accuracy(training batch):0.98000
2016-11-27 13:49:26,499 - DEBUG - Epoch:    19;  cost=0.004719233
2016-11-27 13:49:26,525 - DEBUG - Accuracy(training batch):1.00000
2016-11-27 13:49:46,494 - DEBUG - Epoch:    20;  cost=0.070795499
2016-11-27 13:49:46,513 - DEBUG - Accuracy(training batch):0.97000
2016-11-27 13:49:46,513 - INFO - Optimization Finished!
2016-11-27 13:49:52,636 - INFO - Accuracy - Full Training:0.99094
2016-11-27 13:49:54,214 - INFO - Accuracy - Full Validation:0.97526
2016-11-27 13:51:16,938 - INFO - Training Begins - GradientDescentOptimizer
2016-11-27 13:51:38,348 - DEBUG - Epoch:     1;  cost=3.757446289
2016-11-27 13:51:38,376 - DEBUG - Accuracy(training batch):0.09000
2016-11-27 13:51:44,793 - INFO - Accuracy - Full Training:0.06024
2016-11-27 13:51:46,415 - INFO - Accuracy - Full Validation:0.05900
2016-11-27 13:52:05,922 - DEBUG - Epoch:     2;  cost=3.753748417
2016-11-27 13:52:05,942 - DEBUG - Accuracy(training batch):0.09000
2016-11-27 13:52:25,030 - DEBUG - Epoch:     3;  cost=3.750106812
2016-11-27 13:52:25,049 - DEBUG - Accuracy(training batch):0.09000
2016-11-27 13:52:44,338 - DEBUG - Epoch:     4;  cost=3.746521711
2016-11-27 13:52:44,361 - DEBUG - Accuracy(training batch):0.09000
2016-11-27 13:53:04,437 - DEBUG - Epoch:     5;  cost=3.742992163
2016-11-27 13:53:04,456 - DEBUG - Accuracy(training batch):0.09000
2016-11-27 13:53:26,155 - DEBUG - Epoch:     6;  cost=3.739517450
2016-11-27 13:53:26,181 - DEBUG - Accuracy(training batch):0.09000
2016-11-27 13:53:50,199 - DEBUG - Epoch:     7;  cost=3.736096859
2016-11-27 13:53:50,219 - DEBUG - Accuracy(training batch):0.09000
2016-11-27 13:54:11,955 - DEBUG - Epoch:     8;  cost=3.732729435
2016-11-27 13:54:11,975 - DEBUG - Accuracy(training batch):0.09000
2016-11-27 13:54:31,794 - DEBUG - Epoch:     9;  cost=3.729415178
2016-11-27 13:54:31,815 - DEBUG - Accuracy(training batch):0.09000
2016-11-27 13:54:53,002 - DEBUG - Epoch:    10;  cost=3.726153612
2016-11-27 13:54:53,035 - DEBUG - Accuracy(training batch):0.09000
2016-11-27 13:55:13,720 - DEBUG - Epoch:    11;  cost=3.722943068
2016-11-27 13:55:13,745 - DEBUG - Accuracy(training batch):0.09000
2016-11-27 13:55:35,605 - DEBUG - Epoch:    12;  cost=3.719784260
2016-11-27 13:55:35,624 - DEBUG - Accuracy(training batch):0.09000
2016-11-27 13:55:58,502 - DEBUG - Epoch:    13;  cost=3.716675520
2016-11-27 13:55:58,522 - DEBUG - Accuracy(training batch):0.09000
2016-11-27 13:56:19,752 - DEBUG - Epoch:    14;  cost=3.713616371
2016-11-27 13:56:19,796 - DEBUG - Accuracy(training batch):0.09000
2016-11-27 13:56:44,310 - DEBUG - Epoch:    15;  cost=3.710607052
2016-11-27 13:56:44,332 - DEBUG - Accuracy(training batch):0.09000
2016-11-27 13:57:04,394 - DEBUG - Epoch:    16;  cost=3.707645893
2016-11-27 13:57:04,420 - DEBUG - Accuracy(training batch):0.09000
2016-11-27 13:57:24,559 - DEBUG - Epoch:    17;  cost=3.704732418
2016-11-27 13:57:24,578 - DEBUG - Accuracy(training batch):0.09000
2016-11-27 13:57:44,932 - DEBUG - Epoch:    18;  cost=3.701866388
2016-11-27 13:57:44,958 - DEBUG - Accuracy(training batch):0.09000
2016-11-27 13:58:06,436 - DEBUG - Epoch:    19;  cost=3.699047327
2016-11-27 13:58:06,456 - DEBUG - Accuracy(training batch):0.09000
2016-11-27 13:58:26,366 - DEBUG - Epoch:    20;  cost=3.696274042
2016-11-27 13:58:26,391 - DEBUG - Accuracy(training batch):0.09000
2016-11-27 13:58:26,391 - INFO - Optimization Finished!
2016-11-27 13:58:32,810 - INFO - Accuracy - Full Training:0.06024
2016-11-27 13:58:34,456 - INFO - Accuracy - Full Validation:0.05900
2016-11-27 14:05:18,079 - ERROR - Internal Python error in the inspect module.
Below is the traceback from this internal error.

2016-11-27 14:05:18,106 - INFO - 
Unfortunately, your original traceback can not be constructed.

2016-11-27 14:10:21,068 - INFO - Training Begins - AdamOptimizer
2016-11-27 14:10:46,009 - DEBUG - Epoch:     1;  cost=2.307490826
2016-11-27 14:10:46,037 - DEBUG - Accuracy(training batch): 29.00%
2016-11-27 14:10:52,450 - INFO - Accuracy - Full Training:31.67
2016-11-27 14:10:54,094 - INFO - Accuracy - Full Validation:32.07
2016-11-27 14:11:15,129 - DEBUG - Epoch:     2;  cost=1.542129517
2016-11-27 14:11:15,152 - DEBUG - Accuracy(training batch): 43.00%
2016-11-27 14:11:35,533 - DEBUG - Epoch:     3;  cost=1.373928189
2016-11-27 14:11:35,554 - DEBUG - Accuracy(training batch): 56.00%
2016-11-27 14:11:55,533 - DEBUG - Epoch:     4;  cost=1.199510217
2016-11-27 14:11:55,555 - DEBUG - Accuracy(training batch): 61.00%
2016-11-27 14:12:15,120 - DEBUG - Epoch:     5;  cost=0.790062726
2016-11-27 14:12:15,140 - DEBUG - Accuracy(training batch): 72.00%
2016-11-27 14:12:35,158 - DEBUG - Epoch:     6;  cost=0.521227479
2016-11-27 14:12:35,183 - DEBUG - Accuracy(training batch): 84.00%
2016-11-27 14:12:55,422 - DEBUG - Epoch:     7;  cost=0.286977887
2016-11-27 14:12:55,442 - DEBUG - Accuracy(training batch): 93.00%
2016-11-27 14:13:18,555 - DEBUG - Epoch:     8;  cost=0.252250791
2016-11-27 14:13:18,577 - DEBUG - Accuracy(training batch): 94.00%
2016-11-27 14:13:39,284 - DEBUG - Epoch:     9;  cost=0.196296334
2016-11-27 14:13:39,304 - DEBUG - Accuracy(training batch): 93.00%
2016-11-27 14:14:00,497 - DEBUG - Epoch:    10;  cost=0.189157188
2016-11-27 14:14:00,517 - DEBUG - Accuracy(training batch): 93.00%
2016-11-27 14:14:22,336 - DEBUG - Epoch:    11;  cost=0.191283658
2016-11-27 14:14:22,355 - DEBUG - Accuracy(training batch): 96.00%
2016-11-27 14:14:43,541 - DEBUG - Epoch:    12;  cost=0.143634096
2016-11-27 14:14:43,564 - DEBUG - Accuracy(training batch): 95.00%
2016-11-27 14:15:06,712 - DEBUG - Epoch:    13;  cost=0.106221773
2016-11-27 14:15:06,732 - DEBUG - Accuracy(training batch): 98.00%
2016-11-27 14:15:27,417 - DEBUG - Epoch:    14;  cost=0.110270843
2016-11-27 14:15:27,437 - DEBUG - Accuracy(training batch): 96.00%
2016-11-27 14:15:48,597 - DEBUG - Epoch:    15;  cost=0.068233922
2016-11-27 14:15:48,630 - DEBUG - Accuracy(training batch): 97.00%
2016-11-27 14:16:11,429 - DEBUG - Epoch:    16;  cost=0.025576951
2016-11-27 14:16:11,455 - DEBUG - Accuracy(training batch): 100.00%
2016-11-27 14:16:33,612 - DEBUG - Epoch:    17;  cost=0.036316983
2016-11-27 14:16:33,636 - DEBUG - Accuracy(training batch): 99.00%
2016-11-27 14:16:54,695 - DEBUG - Epoch:    18;  cost=0.013920587
2016-11-27 14:16:54,723 - DEBUG - Accuracy(training batch): 99.00%
2016-11-27 14:17:16,239 - DEBUG - Epoch:    19;  cost=0.017777147
2016-11-27 14:17:16,269 - DEBUG - Accuracy(training batch): 99.00%
2016-11-27 14:17:39,781 - DEBUG - Epoch:    20;  cost=0.026073165
2016-11-27 14:17:39,802 - DEBUG - Accuracy(training batch): 98.00%
2016-11-27 14:17:39,802 - INFO - Optimization Finished!
2016-11-27 14:17:46,757 - INFO - Accuracy - Full Training:99.08
2016-11-27 14:17:48,492 - INFO - Accuracy - Full Validation:97.38
2016-11-27 14:29:47,881 - INFO - Time usage: 0:07:27
2016-11-27 14:31:01,057 - INFO - Training Begins - AdamOptimizer
2016-11-27 14:31:25,152 - INFO - Training Begins - AdamOptimizer - no pool at the first ConvNet
2016-11-27 14:31:44,691 - DEBUG - Epoch:     1;  cost=2.105852604
2016-11-27 14:31:44,724 - DEBUG - Accuracy(training batch): 29.00%
2016-11-27 14:31:50,908 - INFO - Accuracy - Full Training: 34.32%
2016-11-27 14:31:52,551 - INFO - Accuracy - Full Validation: 35.04%
2016-11-27 14:32:10,001 - DEBUG - Epoch:     2;  cost=1.401328921
2016-11-27 14:32:10,020 - DEBUG - Accuracy(training batch): 52.00%
2016-11-27 14:32:28,029 - DEBUG - Epoch:     3;  cost=0.931073070
2016-11-27 14:32:28,048 - DEBUG - Accuracy(training batch): 67.00%
2016-11-27 14:32:46,724 - DEBUG - Epoch:     4;  cost=0.406501323
2016-11-27 14:32:46,742 - DEBUG - Accuracy(training batch): 90.00%
2016-11-27 14:33:04,985 - DEBUG - Epoch:     5;  cost=0.216798529
2016-11-27 14:33:05,004 - DEBUG - Accuracy(training batch): 92.00%
2016-11-27 14:33:23,073 - DEBUG - Epoch:     6;  cost=0.154305592
2016-11-27 14:33:23,092 - DEBUG - Accuracy(training batch): 96.00%
2016-11-27 14:33:41,261 - DEBUG - Epoch:     7;  cost=0.045131344
2016-11-27 14:33:41,280 - DEBUG - Accuracy(training batch): 99.00%
2016-11-27 14:34:01,081 - DEBUG - Epoch:     8;  cost=0.036377020
2016-11-27 14:34:01,101 - DEBUG - Accuracy(training batch): 99.00%
2016-11-27 14:34:19,794 - DEBUG - Epoch:     9;  cost=0.022129202
2016-11-27 14:34:19,813 - DEBUG - Accuracy(training batch): 100.00%
2016-11-27 14:34:38,216 - DEBUG - Epoch:    10;  cost=0.016402047
2016-11-27 14:34:38,235 - DEBUG - Accuracy(training batch): 99.00%
2016-11-27 14:34:58,921 - DEBUG - Epoch:    11;  cost=0.081238844
2016-11-27 14:34:58,940 - DEBUG - Accuracy(training batch): 97.00%
2016-11-27 14:35:17,696 - DEBUG - Epoch:    12;  cost=0.036215659
2016-11-27 14:35:17,715 - DEBUG - Accuracy(training batch): 98.00%
2016-11-27 14:35:36,546 - DEBUG - Epoch:    13;  cost=0.004311120
2016-11-27 14:35:36,565 - DEBUG - Accuracy(training batch): 100.00%
2016-11-27 14:35:55,330 - DEBUG - Epoch:    14;  cost=0.003434789
2016-11-27 14:35:55,350 - DEBUG - Accuracy(training batch): 100.00%
2016-11-27 14:36:14,894 - DEBUG - Epoch:    15;  cost=0.016329059
2016-11-27 14:36:14,920 - DEBUG - Accuracy(training batch): 100.00%
2016-11-27 14:36:37,335 - DEBUG - Epoch:    16;  cost=0.002023319
2016-11-27 14:36:37,354 - DEBUG - Accuracy(training batch): 100.00%
2016-11-27 14:36:56,719 - DEBUG - Epoch:    17;  cost=0.029234042
2016-11-27 14:36:56,739 - DEBUG - Accuracy(training batch): 99.00%
2016-11-27 14:37:16,376 - DEBUG - Epoch:    18;  cost=0.009654955
2016-11-27 14:37:16,395 - DEBUG - Accuracy(training batch): 99.00%
2016-11-27 14:37:35,850 - DEBUG - Epoch:    19;  cost=0.011532084
2016-11-27 14:37:35,869 - DEBUG - Accuracy(training batch): 99.00%
2016-11-27 14:37:54,920 - DEBUG - Epoch:    20;  cost=0.024504965
2016-11-27 14:37:54,938 - DEBUG - Accuracy(training batch): 99.00%
2016-11-27 14:37:54,939 - INFO - Optimization Finished!
2016-11-27 14:38:00,971 - INFO - Accuracy - Full Training: 99.66%
2016-11-27 14:38:02,502 - INFO - Accuracy - Full Validation: 98.25%
2016-11-27 14:38:02,503 - INFO - Time usage: 0:06:37
2016-11-27 14:50:25,466 - INFO - Training Begins - AdamOptimizer - no pool at the first ConvNet
2016-11-27 14:50:25,466 - INFO - Learning Rate: 0.001
2016-11-27 14:50:25,467 - INFO - Batch Size: 100
2016-11-27 14:50:25,468 - INFO - Optimizer: name: "Adam_8"
op: "NoOp"
input: "^Adam_8/update_Variable_52/ApplyAdam"
input: "^Adam_8/update_Variable_53/ApplyAdam"
input: "^Adam_8/update_Variable_54/ApplyAdam"
input: "^Adam_8/update_Variable_55/ApplyAdam"
input: "^Adam_8/update_Variable_56/ApplyAdam"
input: "^Adam_8/update_Variable_57/ApplyAdam"
input: "^Adam_8/update_Variable_58/ApplyAdam"
input: "^Adam_8/update_Variable_59/ApplyAdam"
input: "^Adam_8/Assign"
input: "^Adam_8/Assign_1"

2016-11-27 14:51:02,442 - INFO - Training Begins - AdamOptimizer - no pool at the first ConvNet
2016-11-27 14:51:02,443 - INFO - Learning Rate: 0.001
2016-11-27 14:51:02,443 - INFO - Batch Size: 100
2016-11-27 14:51:02,444 - INFO - Optimizer: name: "Adam_8"
op: "NoOp"
input: "^Adam_8/update_Variable_52/ApplyAdam"
input: "^Adam_8/update_Variable_53/ApplyAdam"
input: "^Adam_8/update_Variable_54/ApplyAdam"
input: "^Adam_8/update_Variable_55/ApplyAdam"
input: "^Adam_8/update_Variable_56/ApplyAdam"
input: "^Adam_8/update_Variable_57/ApplyAdam"
input: "^Adam_8/update_Variable_58/ApplyAdam"
input: "^Adam_8/update_Variable_59/ApplyAdam"
input: "^Adam_8/Assign"
input: "^Adam_8/Assign_1"

2016-11-27 14:52:51,737 - INFO - Training Begins - AdamOptimizer - no pool at the first ConvNet
2016-11-27 14:52:51,739 - INFO - Learning Rate: 0.001
2016-11-27 14:52:51,740 - INFO - Batch Size: 100
2016-11-27 14:52:51,741 - INFO - Optimizer: name: "Adam_8"
op: "NoOp"
input: "^Adam_8/update_Variable_52/ApplyAdam"
input: "^Adam_8/update_Variable_53/ApplyAdam"
input: "^Adam_8/update_Variable_54/ApplyAdam"
input: "^Adam_8/update_Variable_55/ApplyAdam"
input: "^Adam_8/update_Variable_56/ApplyAdam"
input: "^Adam_8/update_Variable_57/ApplyAdam"
input: "^Adam_8/update_Variable_58/ApplyAdam"
input: "^Adam_8/update_Variable_59/ApplyAdam"
input: "^Adam_8/Assign"
input: "^Adam_8/Assign_1"

2016-11-27 14:53:24,798 - INFO - Training Begins - AdamOptimizer - no pool at the first ConvNet
2016-11-27 14:53:24,799 - INFO - Learning Rate: 0.001
2016-11-27 14:53:24,800 - INFO - Batch Size: 100
2016-11-27 14:53:24,801 - INFO - Optimizer: name: "Adam_8"
op: "NoOp"
input: "^Adam_8/update_Variable_52/ApplyAdam"
input: "^Adam_8/update_Variable_53/ApplyAdam"
input: "^Adam_8/update_Variable_54/ApplyAdam"
input: "^Adam_8/update_Variable_55/ApplyAdam"
input: "^Adam_8/update_Variable_56/ApplyAdam"
input: "^Adam_8/update_Variable_57/ApplyAdam"
input: "^Adam_8/update_Variable_58/ApplyAdam"
input: "^Adam_8/update_Variable_59/ApplyAdam"
input: "^Adam_8/Assign"
input: "^Adam_8/Assign_1"

2016-11-27 14:54:01,234 - INFO - Training Begins - AdamOptimizer - no pool at the first ConvNet
2016-11-27 14:54:01,236 - INFO - Learning Rate: 0.001
2016-11-27 14:54:01,236 - INFO - Batch Size: 100
2016-11-27 14:54:01,237 - INFO - Optimizer: name: "Adam_8"
op: "NoOp"
input: "^Adam_8/update_Variable_52/ApplyAdam"
input: "^Adam_8/update_Variable_53/ApplyAdam"
input: "^Adam_8/update_Variable_54/ApplyAdam"
input: "^Adam_8/update_Variable_55/ApplyAdam"
input: "^Adam_8/update_Variable_56/ApplyAdam"
input: "^Adam_8/update_Variable_57/ApplyAdam"
input: "^Adam_8/update_Variable_58/ApplyAdam"
input: "^Adam_8/update_Variable_59/ApplyAdam"
input: "^Adam_8/Assign"
input: "^Adam_8/Assign_1"

2016-11-27 15:00:52,016 - INFO - Training Begins - AdamOptimizer - no pool at the first ConvNet
2016-11-27 15:00:52,017 - INFO - Learning Rate: 0.001
2016-11-27 15:00:52,017 - INFO - Batch Size: 100
2016-11-27 15:00:52,018 - INFO - Learning Rate: 0.001
2016-11-27 15:00:52,019 - INFO - layer_convn1: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:01:17,602 - INFO - Training Begins - AdamOptimizer - no pool at the first ConvNet
2016-11-27 15:01:17,602 - INFO - Learning Rate: 0.001
2016-11-27 15:01:17,603 - INFO - Batch Size: 100
2016-11-27 15:01:17,603 - INFO - Learning Rate: 0.001
2016-11-27 15:01:17,603 - INFO - layer_convn1: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:01:17,604 - INFO - layer_convn2: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:01:17,604 - INFO - layer_fc1: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:01:17,604 - INFO - logits: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:01:40,997 - INFO - ===========================
Training Begins - AdamOptimizer - no pool at the first ConvNet
2016-11-27 15:01:40,998 - INFO - Learning Rate: 0.001
2016-11-27 15:01:40,999 - INFO - Batch Size: 100
2016-11-27 15:01:41,000 - INFO - Learning Rate: 0.001
2016-11-27 15:01:41,000 - INFO - layer_convn1: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:01:41,000 - INFO - layer_convn2: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:01:41,001 - INFO - layer_fc1: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:01:41,001 - INFO - logits: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:02:07,120 - INFO - ===========================
Training Begins - AdamOptimizer  
==================
2016-11-27 15:02:07,120 - INFO - Learning Rate: 0.001
2016-11-27 15:02:07,120 - INFO - Batch Size: 100
2016-11-27 15:02:07,121 - INFO - Learning Rate: 0.001
2016-11-27 15:02:07,121 - INFO - layer_convn1: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:02:07,121 - INFO - layer_convn2: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:02:07,122 - INFO - layer_fc1: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:02:07,122 - INFO - logits: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:02:23,720 - INFO - ===========================
Training Begins - AdamOptimizer  
========================================
2016-11-27 15:02:23,720 - INFO - Learning Rate: 0.001
2016-11-27 15:02:23,721 - INFO - Batch Size: 100
2016-11-27 15:02:23,721 - INFO - Learning Rate: 0.001
2016-11-27 15:02:23,721 - INFO - layer_convn1: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:02:23,721 - INFO - layer_convn2: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:02:23,722 - INFO - layer_fc1: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:02:23,723 - INFO - logits: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:02:32,432 - INFO - ===========================
               Training Begins - AdamOptimizer  
===================================================
2016-11-27 15:02:32,433 - INFO - Learning Rate: 0.001
2016-11-27 15:02:32,433 - INFO - Batch Size: 100
2016-11-27 15:02:32,434 - INFO - Learning Rate: 0.001
2016-11-27 15:02:32,434 - INFO - layer_convn1: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:02:32,435 - INFO - layer_convn2: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:02:32,435 - INFO - layer_fc1: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:02:32,435 - INFO - logits: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:02:39,960 - INFO - ===========================
                                     Training Begins - AdamOptimizer  
===================================================
2016-11-27 15:02:39,961 - INFO - Learning Rate: 0.001
2016-11-27 15:02:39,961 - INFO - Batch Size: 100
2016-11-27 15:02:39,961 - INFO - Learning Rate: 0.001
2016-11-27 15:02:39,962 - INFO - layer_convn1: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:02:39,962 - INFO - layer_convn2: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:02:39,963 - INFO - layer_fc1: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:02:39,963 - INFO - logits: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:02:45,568 - INFO - ===========================
                                     Training Begins - AdamOptimizer  
=========================================================================
2016-11-27 15:02:45,568 - INFO - Learning Rate: 0.001
2016-11-27 15:02:45,569 - INFO - Batch Size: 100
2016-11-27 15:02:45,569 - INFO - Learning Rate: 0.001
2016-11-27 15:02:45,570 - INFO - layer_convn1: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:02:45,570 - INFO - layer_convn2: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:02:45,570 - INFO - layer_fc1: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:02:45,571 - INFO - logits: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:02:47,884 - INFO - ===========================
                                     Training Begins - AdamOptimizer  
=========================================================================
2016-11-27 15:02:47,885 - INFO - Learning Rate: 0.001
2016-11-27 15:02:47,885 - INFO - Batch Size: 100
2016-11-27 15:02:47,885 - INFO - Learning Rate: 0.001
2016-11-27 15:02:47,887 - INFO - layer_convn1: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:02:47,887 - INFO - layer_convn2: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:02:47,888 - INFO - layer_fc1: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:02:47,888 - INFO - logits: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:02:54,165 - INFO - ================================
                                     Training Begins - AdamOptimizer  
=========================================================================
2016-11-27 15:02:54,165 - INFO - Learning Rate: 0.001
2016-11-27 15:02:54,166 - INFO - Batch Size: 100
2016-11-27 15:02:54,166 - INFO - Learning Rate: 0.001
2016-11-27 15:02:54,166 - INFO - layer_convn1: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:02:54,167 - INFO - layer_convn2: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:02:54,167 - INFO - layer_fc1: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:02:54,168 - INFO - logits: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:02:58,338 - INFO - =====================================
                                     Training Begins - AdamOptimizer  
=========================================================================
2016-11-27 15:02:58,338 - INFO - Learning Rate: 0.001
2016-11-27 15:02:58,338 - INFO - Batch Size: 100
2016-11-27 15:02:58,339 - INFO - Learning Rate: 0.001
2016-11-27 15:02:58,339 - INFO - layer_convn1: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:02:58,339 - INFO - layer_convn2: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:02:58,340 - INFO - layer_fc1: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:02:58,340 - INFO - logits: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:03:01,654 - INFO - ========================================
                                     Training Begins - AdamOptimizer  
=========================================================================
2016-11-27 15:03:01,654 - INFO - Learning Rate: 0.001
2016-11-27 15:03:01,655 - INFO - Batch Size: 100
2016-11-27 15:03:01,655 - INFO - Learning Rate: 0.001
2016-11-27 15:03:01,655 - INFO - layer_convn1: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:03:01,655 - INFO - layer_convn2: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:03:01,656 - INFO - layer_fc1: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:03:01,656 - INFO - logits: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:05:59,356 - INFO - ========================================
                                     Training Begins - AdamOptimizer  
=========================================================================
2016-11-27 15:05:59,357 - INFO - Learning Rate: 0.001
2016-11-27 15:05:59,357 - INFO - Batch Size: 100
2016-11-27 15:05:59,357 - INFO - Kernal Size: 5 x 5
2016-11-27 15:05:59,357 - INFO - Stride Parameters Size: [1, 2, 2, 1]
2016-11-27 15:05:59,358 - INFO - Learning Rate: 0.001
2016-11-27 15:05:59,358 - INFO - layer_convn1: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:05:59,358 - INFO - layer_convn2: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:05:59,359 - INFO - layer_fc1: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:05:59,359 - INFO - logits: Tensor("Relu_18:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:07:03,322 - INFO - ========================================
                                     Training Begins - AdamOptimizer  
=========================================================================
2016-11-27 15:07:03,323 - INFO - Learning Rate: 0.0001
2016-11-27 15:07:03,323 - INFO - Batch Size: 100
2016-11-27 15:07:03,324 - INFO - Kernal Size: 5 x 5
2016-11-27 15:07:03,324 - INFO - Stride Parameters Size: [1, 2, 2, 1]
2016-11-27 15:07:03,325 - INFO - layer_convn1: Tensor("Relu_21:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:07:03,325 - INFO - layer_convn2: Tensor("Relu_21:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:07:03,326 - INFO - layer_fc1: Tensor("Relu_21:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:07:03,326 - INFO - logits: Tensor("Relu_21:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:07:23,318 - DEBUG - Epoch:     1;  cost=3.541651726
2016-11-27 15:07:23,357 - DEBUG - Accuracy(training batch): 5.00%
2016-11-27 15:07:29,523 - INFO - ***Accuracy - Full Training:       5.16%
2016-11-27 15:07:31,115 - INFO - ***Accuracy - Full Validation:       5.27%
2016-11-27 15:07:48,452 - DEBUG - Epoch:     2;  cost=3.514196873
2016-11-27 15:07:48,472 - DEBUG - Accuracy(training batch): 5.00%
2016-11-27 15:08:06,079 - DEBUG - Epoch:     3;  cost=3.290594578
2016-11-27 15:08:06,099 - DEBUG - Accuracy(training batch): 15.00%
2016-11-27 15:08:23,432 - DEBUG - Epoch:     4;  cost=2.354758501
2016-11-27 15:08:23,451 - DEBUG - Accuracy(training batch): 24.00%
2016-11-27 15:08:40,696 - DEBUG - Epoch:     5;  cost=2.080152512
2016-11-27 15:08:40,715 - DEBUG - Accuracy(training batch): 36.00%
2016-11-27 15:08:58,859 - DEBUG - Epoch:     6;  cost=1.941978097
2016-11-27 15:08:58,878 - DEBUG - Accuracy(training batch): 42.00%
2016-11-27 15:09:16,114 - DEBUG - Epoch:     7;  cost=1.836625338
2016-11-27 15:09:16,132 - DEBUG - Accuracy(training batch): 41.00%
2016-11-27 15:09:33,873 - DEBUG - Epoch:     8;  cost=1.735201716
2016-11-27 15:09:33,892 - DEBUG - Accuracy(training batch): 43.00%
2016-11-27 15:09:51,537 - DEBUG - Epoch:     9;  cost=1.647466898
2016-11-27 15:09:51,556 - DEBUG - Accuracy(training batch): 47.00%
2016-11-27 15:10:09,681 - DEBUG - Epoch:    10;  cost=1.570447087
2016-11-27 15:10:09,702 - DEBUG - Accuracy(training batch): 53.00%
2016-11-27 15:10:27,204 - DEBUG - Epoch:    11;  cost=1.507112980
2016-11-27 15:10:27,223 - DEBUG - Accuracy(training batch): 53.00%
2016-11-27 15:10:44,700 - DEBUG - Epoch:    12;  cost=1.451485038
2016-11-27 15:10:44,724 - DEBUG - Accuracy(training batch): 53.00%
2016-11-27 15:11:02,308 - DEBUG - Epoch:    13;  cost=1.411975503
2016-11-27 15:11:02,327 - DEBUG - Accuracy(training batch): 57.00%
2016-11-27 15:11:19,935 - DEBUG - Epoch:    14;  cost=1.367476463
2016-11-27 15:11:19,954 - DEBUG - Accuracy(training batch): 55.00%
2016-11-27 15:11:37,515 - DEBUG - Epoch:    15;  cost=1.328722477
2016-11-27 15:11:37,535 - DEBUG - Accuracy(training batch): 56.00%
2016-11-27 15:11:55,072 - DEBUG - Epoch:    16;  cost=1.287948608
2016-11-27 15:11:55,091 - DEBUG - Accuracy(training batch): 57.00%
2016-11-27 15:12:12,610 - DEBUG - Epoch:    17;  cost=1.246698141
2016-11-27 15:12:12,628 - DEBUG - Accuracy(training batch): 58.00%
2016-11-27 15:12:30,239 - DEBUG - Epoch:    18;  cost=1.194474459
2016-11-27 15:12:30,258 - DEBUG - Accuracy(training batch): 60.00%
2016-11-27 15:12:47,772 - DEBUG - Epoch:    19;  cost=1.150175214
2016-11-27 15:12:47,790 - DEBUG - Accuracy(training batch): 61.00%
2016-11-27 15:13:05,456 - DEBUG - Epoch:    20;  cost=1.106765032
2016-11-27 15:13:05,475 - DEBUG - Accuracy(training batch): 62.00%
2016-11-27 15:13:05,475 - INFO - Optimization Finished!
2016-11-27 15:13:11,665 - INFO - ***Accuracy - Full Training:       61.17%
2016-11-27 15:13:13,246 - INFO - ***Accuracy - Full Validation:       60.50%
2016-11-27 15:13:13,247 - INFO - Time usage: 0:06:09
2016-11-27 15:14:12,410 - INFO - ========================================
                                     Training Begins - AdamOptimizer  
=========================================================================
2016-11-27 15:14:12,410 - INFO - Learning Rate: 0.01
2016-11-27 15:14:12,411 - INFO - Batch Size: 100
2016-11-27 15:14:12,411 - INFO - Kernal Size: 5 x 5
2016-11-27 15:14:12,412 - INFO - Stride Parameters Size: [1, 2, 2, 1]
2016-11-27 15:14:12,412 - INFO - layer_convn1: Tensor("Relu_24:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:14:12,413 - INFO - layer_convn2: Tensor("Relu_24:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:14:12,413 - INFO - layer_fc1: Tensor("Relu_24:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:14:12,413 - INFO - logits: Tensor("Relu_24:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:14:31,422 - DEBUG - Epoch:     1;  cost=3.563101530
2016-11-27 15:14:31,454 - DEBUG - Accuracy(training batch): 3.00%
2016-11-27 15:14:37,482 - INFO - ***Accuracy - Full Training:       5.56%
2016-11-27 15:14:39,058 - INFO - ***Accuracy - Full Validation:       5.92%
2016-11-27 15:14:56,403 - DEBUG - Epoch:     2;  cost=3.562325716
2016-11-27 15:14:56,422 - DEBUG - Accuracy(training batch): 3.00%
2016-11-27 15:15:14,421 - DEBUG - Epoch:     3;  cost=3.562185764
2016-11-27 15:15:14,440 - DEBUG - Accuracy(training batch): 3.00%
2016-11-27 15:15:32,294 - DEBUG - Epoch:     4;  cost=3.561096430
2016-11-27 15:15:32,313 - DEBUG - Accuracy(training batch): 3.00%
2016-11-27 15:15:50,134 - DEBUG - Epoch:     5;  cost=3.560094595
2016-11-27 15:15:50,217 - DEBUG - Accuracy(training batch): 3.00%
2016-11-27 15:16:08,110 - DEBUG - Epoch:     6;  cost=3.558801174
2016-11-27 15:16:08,129 - DEBUG - Accuracy(training batch): 3.00%
2016-11-27 15:16:25,929 - DEBUG - Epoch:     7;  cost=3.558464289
2016-11-27 15:16:25,948 - DEBUG - Accuracy(training batch): 3.00%
2016-11-27 15:16:45,234 - DEBUG - Epoch:     8;  cost=3.558238029
2016-11-27 15:16:45,268 - DEBUG - Accuracy(training batch): 3.00%
2016-11-27 15:17:05,774 - DEBUG - Epoch:     9;  cost=3.558106661
2016-11-27 15:17:05,794 - DEBUG - Accuracy(training batch): 3.00%
2016-11-27 15:17:24,408 - DEBUG - Epoch:    10;  cost=3.558058262
2016-11-27 15:17:24,428 - DEBUG - Accuracy(training batch): 3.00%
2016-11-27 15:17:24,428 - INFO - Optimization Finished!
2016-11-27 15:17:30,972 - INFO - ***Accuracy - Full Training:       5.56%
2016-11-27 15:17:32,567 - INFO - ***Accuracy - Full Validation:       5.92%
2016-11-27 15:17:32,567 - INFO - Time usage: 0:03:19
2016-11-27 15:17:40,909 - INFO - ========================================
                                     Training Begins - AdamOptimizer  
=========================================================================
2016-11-27 15:17:40,910 - INFO - Learning Rate: 0.001
2016-11-27 15:17:40,910 - INFO - Batch Size: 100
2016-11-27 15:17:40,911 - INFO - Kernal Size: 5 x 5
2016-11-27 15:17:40,911 - INFO - Stride Parameters Size: [1, 2, 2, 1]
2016-11-27 15:17:40,911 - INFO - layer_convn1: Tensor("Relu_27:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:17:40,912 - INFO - layer_convn2: Tensor("Relu_27:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:17:40,912 - INFO - layer_fc1: Tensor("Relu_27:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:17:40,913 - INFO - logits: Tensor("Relu_27:0", shape=(?, 16, 16, 16), dtype=float32)
2016-11-27 15:18:00,699 - DEBUG - Epoch:     1;  cost=2.070258379
2016-11-27 15:18:00,730 - DEBUG - Accuracy(training batch): 28.00%
2016-11-27 15:18:06,769 - INFO - ***Accuracy - Full Training:       33.96%
2016-11-27 15:18:08,339 - INFO - ***Accuracy - Full Validation:       34.31%
2016-11-27 15:18:25,786 - DEBUG - Epoch:     2;  cost=1.420644641
2016-11-27 15:18:25,805 - DEBUG - Accuracy(training batch): 44.00%
2016-11-27 15:18:43,659 - DEBUG - Epoch:     3;  cost=1.120141506
2016-11-27 15:18:43,679 - DEBUG - Accuracy(training batch): 58.00%
2016-11-27 15:19:01,491 - DEBUG - Epoch:     4;  cost=0.720103979
2016-11-27 15:19:01,510 - DEBUG - Accuracy(training batch): 77.00%
2016-11-27 15:19:19,378 - DEBUG - Epoch:     5;  cost=0.339391708
2016-11-27 15:19:19,397 - DEBUG - Accuracy(training batch): 89.00%
2016-11-27 15:19:37,224 - DEBUG - Epoch:     6;  cost=0.226161659
2016-11-27 15:19:37,243 - DEBUG - Accuracy(training batch): 93.00%
2016-11-27 15:19:56,133 - DEBUG - Epoch:     7;  cost=0.055179391
2016-11-27 15:19:56,157 - DEBUG - Accuracy(training batch): 100.00%
2016-11-27 15:20:15,003 - DEBUG - Epoch:     8;  cost=0.032707028
2016-11-27 15:20:15,022 - DEBUG - Accuracy(training batch): 100.00%
2016-11-27 15:20:36,586 - DEBUG - Epoch:     9;  cost=0.020549979
2016-11-27 15:20:36,611 - DEBUG - Accuracy(training batch): 100.00%
2016-11-27 15:20:58,847 - DEBUG - Epoch:    10;  cost=0.008044925
2016-11-27 15:20:58,872 - DEBUG - Accuracy(training batch): 100.00%
2016-11-27 15:20:58,873 - INFO - Optimization Finished!
2016-11-27 15:21:05,736 - INFO - ***Accuracy - Full Training:       98.95%
2016-11-27 15:21:07,527 - INFO - ***Accuracy - Full Validation:       97.54%
2016-11-27 15:21:07,528 - INFO - Time usage: 0:03:26
